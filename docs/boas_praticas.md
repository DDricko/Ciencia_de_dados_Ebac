# Guia de Boas PrÃ¡ticas - CiÃªncia de Dados

Este documento contÃ©m diretrizes e boas prÃ¡ticas para o desenvolvimento de projetos de ciÃªncia de dados.

## ðŸ“‹ Estrutura de CÃ³digo

### OrganizaÃ§Ã£o de Notebooks

```python
# 1. ImportaÃ§Ãµes
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 2. ConfiguraÃ§Ãµes
pd.set_option('display.max_columns', None)
plt.style.use('seaborn')
sns.set_palette("husl")

# 3. Constantes
RANDOM_STATE = 42
TEST_SIZE = 0.2

# 4. CÃ³digo principal
# ...
```

### ConvenÃ§Ãµes de Nomenclatura

- **VariÃ¡veis**: `snake_case` (ex: `dados_clientes`, `modelo_final`)
- **Constantes**: `UPPER_CASE` (ex: `RANDOM_STATE`, `MAX_FEATURES`)
- **FunÃ§Ãµes**: `snake_case` (ex: `calcular_media()`, `preparar_dados()`)
- **Classes**: `PascalCase` (ex: `ModeloPredicao`, `ProcessadorDados`)

## ðŸ” AnÃ¡lise ExploratÃ³ria de Dados (EDA)

### Checklist BÃ¡sico

1. **VisÃ£o Geral dos Dados**
```python
df.head()
df.info()
df.describe()
df.shape
```

2. **Valores Ausentes**
```python
df.isnull().sum()
df.isnull().sum() / len(df) * 100  # Percentual
```

3. **Valores Ãšnicos**
```python
df.nunique()
df['coluna'].value_counts()
```

4. **EstatÃ­sticas por Grupo**
```python
df.groupby('categoria').agg(['mean', 'median', 'std'])
```

### VisualizaÃ§Ãµes Recomendadas

- **DistribuiÃ§Ãµes**: Histogramas, box plots
- **RelaÃ§Ãµes**: Scatter plots, pair plots
- **CategÃ³ricas**: Bar plots, count plots
- **Temporais**: Line plots
- **CorrelaÃ§Ãµes**: Heatmaps

## ðŸ› ï¸ PrÃ©-processamento de Dados

### Tratamento de Valores Ausentes

```python
# Remover linhas com valores ausentes
df.dropna()

# Preencher com mÃ©dia/mediana
df.fillna(df.mean())

# Preencher com valor especÃ­fico
df.fillna(0)

# Forward/Backward fill (sÃ©ries temporais)
df.fillna(method='ffill')
```

### Tratamento de Outliers

```python
# MÃ©todo IQR
Q1 = df['coluna'].quantile(0.25)
Q3 = df['coluna'].quantile(0.75)
IQR = Q3 - Q1
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

# Filtrar outliers
df_sem_outliers = df[(df['coluna'] >= limite_inferior) & 
                      (df['coluna'] <= limite_superior)]
```

### Encoding de VariÃ¡veis CategÃ³ricas

```python
# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['categoria'])

# Label Encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['categoria_encoded'] = le.fit_transform(df['categoria'])
```

### NormalizaÃ§Ã£o/PadronizaÃ§Ã£o

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# PadronizaÃ§Ã£o (Z-score)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[colunas_numericas])

# NormalizaÃ§Ã£o (0-1)
scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df[colunas_numericas])
```

## ðŸ“Š VisualizaÃ§Ã£o de Dados

### PrincÃ­pios BÃ¡sicos

1. **Clareza**: GrÃ¡ficos devem ser fÃ¡ceis de entender
2. **RelevÃ¢ncia**: Mostrar apenas informaÃ§Ãµes importantes
3. **PrecisÃ£o**: Escala e proporÃ§Ãµes corretas
4. **EstÃ©tica**: Design limpo e profissional

### PersonalizaÃ§Ã£o de GrÃ¡ficos

```python
# ConfiguraÃ§Ã£o padrÃ£o
plt.figure(figsize=(10, 6))
plt.title('TÃ­tulo do GrÃ¡fico', fontsize=14, fontweight='bold')
plt.xlabel('Eixo X', fontsize=12)
plt.ylabel('Eixo Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()
```

## ðŸ¤– Machine Learning

### DivisÃ£o de Dados

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### ValidaÃ§Ã£o Cruzada

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(modelo, X_train, y_train, cv=5)
print(f"AcurÃ¡cia mÃ©dia: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### MÃ©tricas de AvaliaÃ§Ã£o

**ClassificaÃ§Ã£o**:
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
```

**RegressÃ£o**:
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
```

## ðŸ“ DocumentaÃ§Ã£o

### ComentÃ¡rios em CÃ³digo

```python
# BOM: Explicar o "porquÃª"
# Removemos outliers usando IQR pois os dados tÃªm distribuiÃ§Ã£o assimÃ©trica
df_clean = remove_outliers_iqr(df, 'preco')

# RUIM: Explicar o "o quÃª" (Ã³bvio pelo cÃ³digo)
# Remove outliers
df_clean = remove_outliers_iqr(df, 'preco')
```

### Markdown em Notebooks

Use cÃ©lulas markdown para:
- TÃ­tulos de seÃ§Ãµes
- ExplicaÃ§Ãµes de anÃ¡lises
- InterpretaÃ§Ã£o de resultados
- ConclusÃµes e insights

## ðŸ” SeguranÃ§a e Privacidade

1. **Nunca commitar credenciais** (use variÃ¡veis de ambiente)
2. **Anonimizar dados sensÃ­veis** antes de compartilhar
3. **Usar .gitignore** para arquivos de dados grandes
4. **Documentar fontes de dados** e permissÃµes de uso

## ðŸš€ Performance

### Dicas de OtimizaÃ§Ã£o

```python
# Usar tipos de dados apropriados
df['categoria'] = df['categoria'].astype('category')
df['inteiro'] = df['inteiro'].astype('int32')

# OperaÃ§Ãµes vetorizadas (evitar loops)
# RUIM
resultado = [x * 2 for x in df['coluna']]
# BOM
resultado = df['coluna'] * 2

# Usar chunking para arquivos grandes
for chunk in pd.read_csv('arquivo_grande.csv', chunksize=10000):
    processar(chunk)
```

## ðŸ“š Recursos Adicionais

- [PEP 8 - Style Guide for Python Code](https://pep8.org/)
- [Pandas Best Practices](https://pandas.pydata.org/docs/user_guide/gotchas.html)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Data Science Best Practices](https://www.datacamp.com/community/tutorials/data-science-best-practices)

---

**Lembre-se**: CÃ³digo limpo e bem documentado Ã© tÃ£o importante quanto anÃ¡lises corretas!
